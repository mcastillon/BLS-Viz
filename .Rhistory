#### concentrated in smaller areas even though the district's total area is quite large.
#### Let's calculate weighted population density for each congressional district!
cd_tract_density <- read.csv("C:/Users/mcast_000/Documents/Github/Urban Politics/data/cd_tract_density.csv")
colnames(cd_tract_density) <- tolower(colnames(cd_tract_density))
#### We want the census tracts. The target geo id should lead us to the these
count(nchar(cd_tract_density$target.geo.id))
#### 13 characters is congressional districts, 16 for counties, 22 for tracts
tract_density <- subset(cd_tract_density, nchar(target.geo.id)==22)
#### create a congressional district variable
tract_density <- transform(tract_density, cd = substr(target.geo.id, 1, 13))
### looks like we have some water only tracts
count(tract_density$density.land=="(X)")
tract_density <- subset(tract_density, density.land!="(X)")
tract_density$density.land <- as.numeric(tract_density$density.land)
#### We need to create a congressional district variable for merging, but the 2nd and 3rd
#### characters of the target geo id are different between files
head(tract_density$cd)
head(dense_ideal_urb$target.geo.id)
tract_density <- transform(tract_density, cd = sub("11", "00", cd))
cd_weighted_density <- ddply(tract_density, .(cd), summarize,
weighted.population.density = sum(population*density.land) / sum(population))
dense_ideal_urb_weight <- merge(dense_ideal_urb, cd_weighted_density,
by.x="target.geo.id", by.y="cd", all.x=T)
#### Now let's see if we improve the model at all with the weighted density metrics
detach(dense_ideal_urb)
attach(dense_ideal_urb_weight)
rownames(dense_ideal_urb_weight) <- label
weight_lm <- lm(idealPoint ~ weighted.population.density)
summary(weight_lm)
#### so far looks good, but let's plot
plot(weighted.population.density, idealPoint)
abline(weight_lm)
#### We should log transform the weighted population density
dense_ideal_urb_weight <- transform(dense_ideal_urb_weight,
log.weighted.density = log(weighted.population.density))
detach(dense_ideal_urb_weight)
attach(dense_ideal_urb_weight)
hist(weighted.population.density)
hist(log.weighted.density)
#### This model looks like it performs quite well
weight_log_lm <- lm(idealPoint ~ log.weighted.density)
summary(weight_log_lm)
#### But we should still account for political party
plot(log.weighted.density, idealPoint)
abline(weight_log_lm)
#### It appears we still have the Democratic-only phenomenon
ggplot(data=dense_ideal_urb_weight, aes(log.weighted.density, idealPoint, colour=factor(party))) +
geom_point() + geom_smooth() + scale_color_manual(values=alpha(c("blue", "red"), .4))
weight_lm_party <- lmList(data=dense_ideal_urb_weight,
idealPoint ~ log.weighted.density | factor(party))
summary(weight_lm_party)
#### compared with the log population density model
summary(dense_lm_log_party)
#### Based off of the residual standard errors of the models, it appears the weighted
#### density model performs better. We even have a GOP density coefficient that is
#### statistically significant
#### So let's take another close look at Dems
d_weight_fitted <- fortify(weight_lm_party$D)
ggplot(data=d_weight_fitted, aes(log.weighted.density, idealPoint)) +
geom_point() + geom_smooth()
detach(dense_ideal_urb_weight)
attach(d_weight_fitted)
#### which democrats' voting patterns are least representative of their district's
#### weighted density?
d_weight_fitted <- transform(d_weight_fitted, extreme = ifelse(abs(.stdresid)>1.5,1,0))
d_weight_fitted$label <- rownames(d_weight_fitted)
ggplot(data=d_weight_fitted, aes(log.weighted.density, idealPoint)) +
geom_point() + geom_smooth() +
geom_text(data=d_weight_fitted[d_weight_fitted$extreme == 1,],
aes(label=label, size=abs(.stdresid), angle=45)) +
scale_size(range=c(3,5))
detach(d_weight_fitted)
#### Let's see how the "extremes" compare
extreme_comp_d <- merge(d_weight_fitted, d_fitted,
by="label", all=T, suffixes=c("_weight", "_regular"))
#### 7 extreme in old only, 9 in weighted only
count(extreme_comp_d[,c("extreme_weight", "extreme_regular")])
extreme_reg_only_d <- subset(extreme_comp_d, extreme_weight==0 & extreme_regular==1)
arrange(extreme_reg_only_d, .stdresid_regular)
#### These representatives will be in districts that have weighted population densities
#### that are more in line with their ideology than their regular population densities.
#### The representatives that are no longer extreme examples include:
#### Grijalva - AZ-3 (liberal in large swath of Arizona desert)
#### Murphy - FL-18 (Moderate in suburban Miami)
#### McIntyre - NC-7 (moderate in the southern part of North Carolina which is relatively not dense throughout)
#### Nadler - NY-10 (very liberal in very dense Manhattan)
extreme_weight_only_d <- subset(extreme_comp_D, extreme_weight==1 & extreme_regular==0)
arrange(extreme_weight_only_d, .stdresid_weight)
#### Here we have representatives whose ideologies match their population density, but not
#### their weighted population density. They include:
#### Cuellar - TX-28 (Moderate in a large district with population concentrated largely in San Antonio)
#### Barber - AZ-2 (Moderate with most of district in Tucson, but most area desert)
#### Lewis - GA-5 (Liberal in sprawling Atlanta)
#### Pallone - NJ-6 (Progressive in Suburban New Jersey)
#### One definite takeaway is that population density can be a slightly misleading
#### statistic. Weighted population density better takes into account a district's makeup
#### how about them republicans?
r_weight_fitted <- fortify(weight_lm_party$R)
ggplot(data=r_weight_fitted, aes(log.weighted.density, idealPoint)) +
geom_point() + geom_smooth()
attach(r_weight_fitted)
#### which republicans' voting patterns are least representative of their district's
#### weighted density?
r_weight_fitted <- transform(r_weight_fitted, extreme = ifelse(abs(.stdresid)>1.5,1,0))
r_weight_fitted$label <- rownames(r_weight_fitted)
ggplot(data=r_weight_fitted, aes(log.weighted.density, idealPoint)) +
geom_point() + geom_smooth() +
geom_text(data=r_weight_fitted[r_weight_fitted$extreme == 1,],
aes(label=label, size=abs(.stdresid), angle=45)) +
scale_size(range=c(3,5))
detach(r_weight_fitted)
#### I am hesitant to draw many conclusions from the Republican model since the
#### relationship between density and ideology is less pronounced. However, it is
#### certainly fascinating that the 2 most moderate Republican congressmen are from
#### relatively rural areas (Gibson - NY-19 and Jones - NC-3). These reps are potentially
#### vulnerable to primary challenges.
#### Also of note, many of the congressmen (Dems and GOP) who are more conservative
#### relative to their density are from Texas, while many who are more liberal are found
#### in New York/Jersey. This makes me believe that another variable is necessary to
#### estimate ideology. Based on what I know about these states demographics I think
#### educational attainment is an important contributing factor
#### Let's grab a dataset from American Fact Finder
ed_attain <- read.csv("C:/Users/mcast_000/Documents/Github/Urban Politics/data/ed_attain.csv")
colnames(ed_attain) <- tolower(colnames(ed_attain))
colnames(ed_attain)
#### There are a lot of variables, but tthe total estimates are of most interest to us
#### We can make use of the dplyr package to grab the necessary vars
library(dplyr)
ed_attain <- select(ed_attain, starts_with("id"), geography, starts_with("total"),
-contains("error"), -contains("earnings"), -contains("poverty"))
colnames(ed_attain) <- sub("total..estimate..", "te_", colnames(ed_attain))
dense_ideal_urb_weight_ed <- merge(dense_ideal_urb_weight, ed_attain,
by.x="Geography", by.y="geography",
all.x=T)
# library(penalized)
library(lars)
# pen_mod <- penalized(data = dense_ideal_urb_weight_ed,
#                      response = idealPoint,
#                      penalized = ~ log.weighted.density +
#                                   te_less.than.9th.grade +
#                                   te_9th.to.12th.grade..no.diploma +
#                                   te_high.school.graduate..includes.equivalency..1 +
#                                   te_some.college..no.degree +
#                                   te_associate.s.degree +
#                                   te_bachelor.s.degree +
#                                   te_graduate.or.professional.degree,
#                      model = "linear", fusedl=T,
#                      lambda1 = 1, lambda2 = 2, steps=20)
lar_mod <- lars(x = dense_ideal_urb_weight_ed[, c("log.weighted.density",
"te_less.than.9th.grade",
"te_9th.to.12th.grade..no.diploma",
"te_high.school.graduate..includes.equivalency..1",
"te_associate.s.degree",
"te_graduate.or.professional.degree")]
"te_bachelor.s.degree",
"te_some.college..no.degree",
y = dense_ideal_urb_weight_ed$idealPoint,
type = "lasso")
lar_mod <- lars(x = dense_ideal_urb_weight_ed[, c("log.weighted.density",
"te_less.than.9th.grade",
"te_9th.to.12th.grade..no.diploma",
"te_high.school.graduate..includes.equivalency..1",
"te_some.college..no.degree",
"te_associate.s.degree",
"te_bachelor.s.degree",
"te_graduate.or.professional.degree")]
y = dense_ideal_urb_weight_ed$idealPoint,
type = "lasso")
lar_mod <- lars(x = dense_ideal_urb_weight_ed[, c("log.weighted.density",
"te_less.than.9th.grade",
"te_9th.to.12th.grade..no.diploma",
"te_high.school.graduate..includes.equivalency..1",
"te_some.college..no.degree",
"te_associate.s.degree",
"te_bachelor.s.degree",
"te_graduate.or.professional.degree")],
y = dense_ideal_urb_weight_ed$idealPoint,
type = "lasso")
traceback()
lars
library(penalized)
pen_mod <- penalized(data = dense_ideal_urb_weight_ed,
response = idealPoint,
penalized = ~ log.weighted.density +
te_less.than.9th.grade +
te_9th.to.12th.grade..no.diploma +
te_high.school.graduate..includes.equivalency..1 +
te_some.college..no.degree +
te_associate.s.degree +
te_bachelor.s.degree +
te_graduate.or.professional.degree,
model = "linear", fusedl=T,
lambda1 = 1, lambda2 = 2, steps=20)
summary(pen_mod)
show(pen_mod)
coef(pen_mod)
coefficients(pen_mod)
?penalized
plotpath(pen_mod)
pen_mod <- penalized(data = dense_ideal_urb_weight_ed,
response = idealPoint,
penalized = ~ log.weighted.density +
te_less.than.9th.grade +
te_9th.to.12th.grade..no.diploma +
te_high.school.graduate..includes.equivalency..1 +
te_some.college..no.degree +
te_associate.s.degree +
te_bachelor.s.degree +
te_graduate.or.professional.degree,
model = "linear", lambda2 = 2, steps=20)
show(pen_mod)
plotpath(pen_mod)
update.packages()
ls()
q()
sample(65,1)
sample(2,1)
sample(22,1)
update.packages()
ls()
library(knitr)
library(tools)
names(vignetteEngine(package = "knitr"))
devtools::use_vignette("my-vignette")
library(devtools)
install.packages("devtools")
devtools::use_vignette("my-vignette")
library(devtools)
devtools::use_vignette("my-vignette")
install.packages("Rtools")
update
updateR
library(installR)
library(installr)
install.packages("installr")
library(installr)
updateR
updateR()
data(precip)
precip_data <- precip[c("Washington", "Nashville", "Los Angeles", "Boston", "New York")]
precip_data
plot(precip_data)
q()
rm(list=ls())
q()
sample(26,1)
sample(11,1)
update.packages()
data(USArrests)
USArrests
q()
q()
rm(list=ls())
q()
q()
q()
update.packages()
q()
sample(20,1)
sample(6),1)
sample(6,1)
sample(9,1)
sample(13,1)
sample(19,1)
sample(10,1)
sample(6,1)
install.packages(c("deldir", "magrittr", "mgcv"))
update.packages()
update.packages()
ls()
q()
q()
update.packages()
q()
sample(9,1)
sample(10,1)
sample(11,1)
update.packages()
q()
sample(2,1)
sample(2,1)
sample(2,1)
sample(2,25,rep=T)
q()
q()
sample(119,1)
update.packages()
update.packages()
84.18/5.5
34.97/2.5
getwd()
update.packages()
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
mydata <- sasxport.get("C:\\Users\\mcast_000\\Downloads\\MCQ_G.xpt")
mydata
head(MCQ_G)
head(mydata)
dia <- sasxport.get("C:\\Users\\mcast_000\\Downloads\\diq_G.xpt")
head(data)
head(dia)
install.packages("rvest")
sample(149,1)
sample(149,1)
sample(9,1)
sample(13,1)
41.3*20
34.42*24
27.54*30
27.54*24
34.42*18
41.3*12
q()
sample(36,1)
sample(9,1)
sample(3,1)
sample(2,1)
sample(149,1)
sample(8,1)
update.packages()
q()
sample(5,7,rep=T)
update.packages()
sample(11,1)
sample(10,1)
sample(11,1)
sample(10,1)
sample(9,1)
sample(8,1)
sample(7,1)
sample(6,1)
sample(5,1)
sample(4,1)
sample(2,1)
update.packages()
q()
update.packages()
q()
q()
q()
sample(5,10,rep=T)
update.packages()
sample(26,1)
sample(25,1)
sample(26,1)
sample(22,1)
sample(20,1)
update.packages()
sample(26,1)
sample(23,1)
q()
update.packages()
setwd("C:/Users/mcast_000/Documents/GitHub/BLS Viz")
options(stringsAsFactors = F)
library(data.table)
bls_occ <- fread("Data/bls_occ.csv")
colnames(bls_occ)
library(plyr)
library(dplyr)
bls_occ <- data.table(bls_occ)
bls_occ
### area_type==4 gives us metro areas
bls_occ_msa <- filter(bls_occ, area_type==4)
### occupation category
bls_occ_msa[, occ_cat := substr(occ_code, 1, 2)]
setkey(bls_occ_msa, occ_cat)
### a few different kinds of groups
count(bls_occ_msa, group)
### we'll take the major occupation group and the associated title
occ_groups <- bls_occ_msa %>%
filter(group == "major") %>%
select(occ_title, occ_cat) %>%
unique() %>%
setnames("occ_title", "major_occ")
setkey(occ_groups, occ_cat)
### and add it to the original dataset
bls_occ_msa <- bls_occ_msa[occ_groups]
### and filter to detailed groups
library(magrittr)
bls_occ_msa %<>% filter(group == "detail")
bls_occ_msa
bls_occ_msa %<>% select(area_title, occ_code, occ_title, tot_emp, jobs_1000,
                  loc_quotient, starts_with("h_"), starts_with("a_"),
                        major_occ)
bls_occ_msa %<>%
  mutate_each_(funs(as.numeric), c("tot_emp", "jobs_1000", "loc_quotient"))
ah_vars <- colnames(select(bls_occ_msa, starts_with("h_"), starts_with("a_")))
bls_occ_msa %<>% mutate_each_(funs(as.numeric), ah_vars)
library(reshape2)
occ_rs <- recast(bls_occ_msa, id.var=c("area_title", "major_occ"),
                 measure.var="tot_emp", fun.aggregate=sum,
                 fill=0, area_title ~ major_occ)
occ_rs[is.na(occ_rs)] <- 0
occ_rs <- data.table(occ_rs)
occ_rs[, tot_emp := rowSums(select(occ_rs, -area_title))]
occ_msa_scale <- select(occ_rs, -area_title, -tot_emp) / occ_rs$tot_emp
rownames(occ_msa_scale) <- occ_msa_scale$area_title
occ_dis <- dist(occ_msa_scale, "euclidean")
hc_occ <- hclust(occ_dis, method = "ward.D2")
hc_occ$labels <- rownames(occ_msa_scale)
occ_clus <- cutree(hc_occ, 14)
library(ade4)
hcp_occ <- hclust2phylog(hc_occ)
radial.phylog(hcp_occ)
library(rjson)
HCtoJSON <- function(hc){
  labels <- hc$labels
  clus <- hc$clus
  merge<-data.frame(hc$merge)
  for (i in (1:nrow(merge))) {
    if (merge[i,1]<0 & merge[i,2]<0) {eval(parse(text=paste0("node", i, "<-list(name=\"node", i, "\", children=list(list(name=labels[-merge[i,1]]),list(name=labels[-merge[i,2]])))")))}
    else if (merge[i,1]>0 & merge[i,2]<0) {eval(parse(text=paste0("node", i, "<-list(name=\"node", i, "\", children=list(node", merge[i,1], ", list(name=labels[-merge[i,2]])))")))}
    else if (merge[i,1]<0 & merge[i,2]>0) {eval(parse(text=paste0("node", i, "<-list(name=\"node", i, "\", children=list(list(name=labels[-merge[i,1]]), node", merge[i,2],"))")))}
    else if (merge[i,1]>0 & merge[i,2]>0) {eval(parse(text=paste0("node", i, "<-list(name=\"node", i, "\", children=list(node",merge[i,1] , ", node" , merge[i,2]," ))")))}
  }
  eval(parse(text=paste0("JSON<-toJSON(node",nrow(merge), ")")))
  return(JSON)
}
hc_occ_json <- HCtoJSON(hc_occ)
cat(hc_occ_json, file="src/hc_occ.json")
biz_pat_naics <- fread("Data/BP_2012_00A1_with_ann.csv")
setnames(biz_pat_naics, tolower(colnames(biz_pat_naics)))
names(biz_pat_naics)
setnames(biz_pat_naics, gsub("-", ".", colnames(biz_pat_naics)))
biz_pat_naics
biz_pat_naics %<>%
  mutate(emp = ifelse(emp=="a", 10,
                      ifelse(emp=="b", 60,
                             ifelse(emp=="c", 175,
                                    ifelse(emp=="e", 375,
                                           ifelse(emp=="f", 750,
                                                  ifelse(emp=="h", 3750,
                                                         ifelse(emp=="g", 1750,
                                                                ifelse(emp=="k", 375000,
                                                                       ifelse(emp=="j", 17500,
                                                                              ifelse(emp=="i", 7500, emp)))))))))))
biz_pat_naics %<>% mutate(emp = as.numeric(emp))
count(biz_pat_naics, naics.display.label)
count(biz_pat_naics, geo.display.label)
library(stringr)
biz_pat_naics %<>% filter(grep("Metro Area", geo.display.label))
biz_pat_naics[, area_title := str_replace(geo.display.label, " Metro Area", "")]
biz_pat_areas <- count(biz_pat_naics, area_title)
setkey(biz_pat_areas, area_title)
unkey(biz_pat_areas)
setkey(biz_pat_areas, NULL)
bls_areas <- count(bls_occ_msa, area_title)
overlap_areas <- inner_join(biz_pat_areas, bls_areas, by = "area_title")
nrow(overlap_areas)
nrow(biz_pat_ares)
nrow(biz_pat_areas)
nrow(bls_areas)
nrow(overlap_areas)
naics_rs <- recast(biz_pat_naics, id.var=c("area_title", "naics.display.label"),
                   measure.var="emp", fun.aggregate=sum,
                   fill=0, area_title ~ naics.display.label)
naics_rs <- data.table(naics_rs)
naics_rs[, naics_emp := rowSums(select(naics_rs, -area_title))]
naics_msa_scale <- select(naics_rs, -area_title, -naics_emp) / naics_rs$naics_emp
rownames(naics_msa_scale) <- naics_msa_scale$area_title
naics_dis <- dist(naics_msa_scale, "euclidean")
hc_naics <- hclust(naics_dis, method = "ward.D2")
hc_naics$labels <- rownames(naics_msa_scale)
naics_clus <- cutree(hc_naics, 14)
naics_clus
hc_naics$labels <- rownames(naics_msa_scale)
naics_clus <- cutree(hc_naics, 14)
naics_clus
rownames(naics_msa_scale) <- naics_msa_scale$area_title
naics_dis <- dist(naics_msa_scale, "euclidean")
hc_naics <- hclust(naics_dis, method = "ward.D2")
hc_naics$labels <- rownames(naics_msa_scale)
naics_clus <- cutree(hc_naics, 14)
naics_clus
occ_clus
rownames(occ_msa_scale) <- occ_msa_scale$area_title
hc_occ$labels <- rownames(occ_msa_scale)
occ_clus <- cutree(hc_occ, 14)
occ_clus
rownames(occ_msa_scale)
occ_msa_scale$area_title
rownames(occ_msa_scale) <- occ_rs$area_title
hc_occ$labels <- rownames(occ_msa_scale)
occ_clus <- cutree(hc_occ, 14)
occ_clus
rownames(naics_msa_scale) <- naics_rs$area_title
hc_naics$labels <- rownames(naics_msa_scale)
naics_clus <- cutree(hc_naics, 14)
naics_clus
dim(naics_clus)
attributes(naics_clus)
naics_clus==3
filter(naics_clus==3)
subset(naics_clus==3)
occ_clus$clus <- occ_clus
occ_clus
occ_clus <- cutree(hc_occ, 14)
?cutree
subset(occ_clus, 3)
subset(occ_clus==3)
lapply(1:14, function(i) {
occ_clus[occ_clus==i]
})
lapply(1:14, function(i) {
naics_clus[naics_clus==i]
})
hcp_naics <- hclust2phylog(hc_naics)
cat(hc_naics_json, file="src/hc_naics.json")
hc_naics_json <- HCtoJSON(hc_naics)
cat(hc_naics_json, file="src/hc_naics.json")
q()
